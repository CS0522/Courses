<!-- Authorized by Frank -->
- [01 什么是机器学习](#01-什么是机器学习)
    - [人工智能、机器学习、深度学习 (P11)](#人工智能机器学习深度学习-p11)
    - [机器学习とは (P15)](#机器学习とは-p15)
    - [机器学习的分类 (P16)](#机器学习的分类-p16)
- [02](#02)
- [03](#03)
- [04](#04)
- [05](#05)
- [06](#06)
- [07](#07)
- [08](#08)
- [09](#09)
- [10](#10)
- [11](#11)
- [12 Pattern Mining](#12-pattern-mining)
    - [Pattern Mining とは (P5)](#pattern-mining-とは-p5)
    - [Apriori 定理](#apriori-定理)
    - [Apriori Algorithm (P6)](#apriori-algorithm-p6)
    - [FP - Growth Algorithm (P18)](#fp---growth-algorithm-p18)
    - [推荐 system 中的学习 (P24)](#推荐-system-中的学习-p24)
- [13 系列 Data 识别](#13-系列-data-识别)
  - [系列 Data (P5)](#系列-data-p5)
  - [系列 Data 识别问题分类 (P5)](#系列-data-识别问题分类-p5)
    - [1. 入力、出力等长 (系列标签问题, P9)](#1-入力出力等长-系列标签问题-p9)
      - [对数线性模型 (P11)](#对数线性模型-p11)
      - [CRF 条件付き確率場 (P14)](#crf-条件付き確率場-p14)
    - [2. 入力不定、出力长度为 1 (系列识别问题, P15)](#2-入力不定出力长度为-1-系列识别问题-p15)
      - [HMM 隐藏马尔科夫模型 (P17)](#hmm-隐藏马尔科夫模型-p17)
      - [系列识别问题的例子 (P15)](#系列识别问题的例子-p15)
    - [3. 入力出力之间没有明确关系](#3-入力出力之间没有明确关系)
- [14 强化学习](#14-强化学习)
  - [强化学习 (P5)](#强化学习-p5)
  - [马尔科夫决定过程 (P6)](#马尔科夫决定过程-p6)
  - [1 状态问题的定式化 - K armed bandit (P7)](#1-状态问题的定式化---k-armed-bandit-p7)
    - [决定状况的定式化 (P8)](#决定状况的定式化-p8)
    - [非决定情况的定式化 (P9)](#非决定情况的定式化-p9)
  - [马尔科夫决定过程的定式化 (P10)](#马尔科夫决定过程的定式化-p10)
  - [Bellman 方程式 \& Q值的推导 (P14)](#bellman-方程式--q值的推导-p14)
  - [Q值的推定方法 (P15)](#q值的推定方法-p15)
    - [model base 手法 (P16)](#model-base-手法-p16)
    - [model free 手法 (P18)](#model-free-手法-p18)
      - [TD 时间差分学习 (P18)](#td-时间差分学习-p18)
      - [决定的 TD 学习 (P19)](#决定的-td-学习-p19)
      - [概率的 TD 学习 (P22)](#概率的-td-学习-p22)
- [15 半教師あり学習](#15-半教師あり学習)
  - [半教師あり学習 (P4)](#半教師あり学習-p4)
    - [1. 数值特征的情况 (P6)](#1-数值特征的情况-p6)
      - [适合的数据 (P6)](#适合的数据-p6)
      - [不适合的数据 (P7)](#不适合的数据-p7)
      - [半教師あり学習が可能な data の仮定 (P8)](#半教師あり学習が可能な-data-の仮定-p8)
    - [2. category 特征的情况 (P9)](#2-category-特征的情况-p9)
      - [适合的数据 (P9)](#适合的数据-p9)
      - [Overlap 的传播 (P10)](#overlap-的传播-p10)
    - [半教師あり学習 Algorithm (P11)](#半教師あり学習-algorithm-p11)
  - [自己学习 Self-Training (P13)](#自己学习-self-training-p13)
  - [共训练 Co-Training (P14)](#共训练-co-training-p14)
  - [YATSI Algorithm Yet Another Two-Stage Idea (P16)](#yatsi-algorithm-yet-another-two-stage-idea-p16)
  - [Label 传播法 (P18)](#label-传播法-p18)


# 01 什么是机器学习

### 人工智能、机器学习、深度学习 (P11)
* 人工智能 > 机器学习 > 深度学习
* 人工智能 (P11)
  * 探索
  * 推论
  * 知识表现
* 机器学习 (P12)
  * SVM
  * 勾配 boosting
* 深度学习 (P13)
  * CNN (卷积神经网络)
  * RNN (循环神经网络)

### 机器学习とは (P15)
明示的に解法が与えられていない task を遂行のため、model を学习数据から構築する
* task: 机器学习的处理对象
* 学习数据: 用于构建模型的数据

### 机器学习的分类 (P16)
* 教師あり学習 (P17)
  * 识别 (P18): category data
  * 回归 (P19): 数值 data
* 教師なし学習 (P20)
  * model 推定 (P21): 全体支配的规则性
  * pattern mining (模式挖掘) (P22): 部分集合内、集合间成立的规则性
* 中間的学習 (P23)
  * 半教師あり学習 (P24)
  * 強化学習 (P25)




# 02




# 03




# 04




# 05 




# 06




# 07




# 08




# 09




# 10




# 11




# 12 Pattern Mining

### Pattern Mining とは (P5)
* 頻出項目抽出: pattern を抽出
* 連想規則抽出: 根据 pattern 查找规则

### Apriori 定理
* 频繁集 (P8): 项集的支持度超过设定的阈值
* 対偶 (P9): 逆否命题
* 如果一个集合是频繁集，它的子集也是频繁集；如果一个集合不是频繁集，它的超集也不是频繁集 (P10)
* 如果某个结论规则的置信度高的话，它的子集的置信度也高；如果某个结论规则的置信度低的话，他的超集的置信度也低 (P15)

### Apriori Algorithm (P6)
* 利用 Apriori 定理的逆否命题，从小项目集合开始计算支持度，不扩展不频繁出现的集合，减少要调查的项目集合
* 频出项目抽出 (P6)
  * transaction (P6): 一条购买记录构成一个 transaction，也就是一条数据
  * 支持度 (P6)

* 连想规则抽出 (P12)
  * 置信度 (P13)
  * lift 值 (P13)
  * 作成手顺: (P12)
    * 抽出频繁集
    * 集合分成条件部、结论部，生成可能的规则集合
    * 评估规则的有用性，并缩小可能有用的范围

### FP - Growth Algorithm (P18)
* 高速化 Apriori 算法
* 将 transaction data 转换为 `compact情報` (紧凑信息) 并对该信息进行 pattern mining
* compact 手順 (P18)
* FP - Growth Algorithm の例 (P19 - 22)
* 对 FP 木进行 pattern mining (P23)

### 推荐 system 中的学习 (P24)
* 协调 filtering
* Matrix Factorization
  * Alternating Least Squares
  * Non-negative Matrix Factorization




# 13 系列 Data 识别

## 系列 Data (P5)
各个要素之间相互独立，无依赖关系

## 系列 Data 识别问题分类 (P5)

### 1. 入力、出力等长 (系列标签问题, P9)
* 例: 形态素解析，一个词语对应一个品词
* point: 输入输出前后存在依赖关系 (P6)
* 系列数据一个一个进行识别の問題点
  * 1: 系列の性質を捨ててしまう
  * 2: 膨大な class 数

#### 对数线性模型 (P11)
* 将前面和后面的输入输出自由组合为特征向量的元素，反映一个序列
* 可以解决问题1

#### CRF 条件付き確率場 (P14)
* 输出序列只参照前面一个元素，而输入序列自由范围参照
* 遷移素性 (P13): 出力系列参照的素性
* 観測素性 (P13): 入力系列对应的素性
* ビタビ Algorithm (P14): 求最大值

### 2. 入力不定、出力长度为 1 (系列识别问题, P15)
* 例: 识别出特定的分类
* point: (P7)
  * 输入数据长度不定
  * 输入数据有共同性质，但是如何分割是不确定的
  * 教師なし学習と教師あり学習を組み合わせ

#### HMM 隐藏马尔科夫模型 (P17)
* 遷移確率 (P17): 状态 i 到状态 j 的转移概率 a<sub>ij</sub>
* 出力確率 (P17): 在状态 i 时的输出 o 的概率 b<sub>i</sub>(o)

#### 系列识别问题的例子 (P15)
通过判断用户的 Key 和 mouse 的输入操作，识别是新手还是熟练
* EM 算法，不断计算 P(**x**) 和 HMM 的参数 a<sub>ij</sub>, b<sub>i</sub>(o) (P18)
* 通过 ビタビ Algorithm ，求得相对于输入序列，概率最大的转移序列 (P19)
* 最大概率的转移序列确定，全部隐藏变量的位置也就确定，每个输入的子序列的输出也是确定的 (P19)

### 3. 入力出力之间没有明确关系 




# 14 强化学习 

## 强化学习 (P5)
* 为了获得报酬，对环境进行执行操作的 `意思決定 agent` 的学习
* 意思决定 agent: 决策代理，如 robot、象棋运行的程序
* agent 以获得更多的报酬为目的 (是个函数)
  * 输入: 状态或状态的概率分布
  * 输出: 行为
* 利用 马尔科夫决定过程 (MDP)

## 马尔科夫决定过程 (P6)
假定了以下的条件
* 环境是离散的状态集合
* 在时刻t，状态是 `st`，agent 执行了行为 `at` 后，获得报酬 `r(t+1)`，状态迁移至 `s(t+1)`
* 状态迁移是概率性的，该概率只依赖于迁移前的状态

## 1 状态问题的定式化 - K armed bandit (P7)
马尔科夫决定过程里最简单的例子
* 1 状态: 只有一台 slot machine
* K armed: 有多个 arm 把手，也就是有 K 个选择
* 报酬: 立即给予

### 决定状况的定式化 (P8)
* 所有的情况一一进行尝试，选择报酬最高的作为学习结果

### 非决定情况的定式化 (P9)
* 行为a对应的报酬r是服从概率 p(r|a)的
* 多次尝试，选取平均报酬多的作为结果

## 马尔科夫决定过程的定式化 (P10)
* 报酬和迁移到下一个状态的概率，只与现在的状态和行为有关
* 评价一个策略 pi 的好坏，根据策略执行时累积的报酬期待值 `V(st)` 进行评价 (P11)
* 学习的目标，就是获得最优的策略 pi (P12)
* 最优策略: 对于所有状态，累积报酬的期待值达到最大的策略 (P12)

## Bellman 方程式 & Q值的推导 (P14)

## Q值的推定方法 (P15)

### model base 手法 (P16)
环境 model 化，给定了 `状态迁移概率` 和 `报酬的概率分布`。考虑使用动态规划法
* 通过 `value iteration Algorithm`，求出状态评价函数V(s)的最优值（Q值最大）
* `value iteration Algorithm` (P17)
  * 第 1 回，获得 goal 前一个状态的值
  * 第 2 回，获得 goal 前两个状态的值，同时第 1 回的值还要更新

### model free 手法 (P18)
状态迁移概率和报酬的概率分布未知，`試行錯誤` を通じて環境と相互作用をした結果を使って学習する

#### TD 时间差分学习 (P18)
* 将 `e - greedy` 法作为探索策略
  * 最适行为: 概率 1-e
  * 其他行为: 概率 e
  * 实际上通过以下式子进行行动的选择 (P18)

#### 决定的 TD 学习 (P19)
报酬和迁移未知，但是已经决定 (?)
* 此时，在 bellman 方程式中，将概率去除
* 例子: P21

#### 概率的 TD 学习 (P22)
Q值按照一定的比例更新，这个比例随着时间而减少
* P22




# 15 半教師あり学習 

## 半教師あり学習 (P4)
* 有数值形式、category 形式，教師あり、教師なし data 混杂在一起
* 通过有正确答案的数据构建判别器，没有正确答案的用来提高判别器的性能
* 实际上是少量的有正确答案的数据、大量的没有正确答案的数据

### 1. 数值特征的情况 (P6)

#### 适合的数据 (P6)
* 没有混杂，界限分明
* 可以通过有正确答案的分布，按比例准确估计没有正确答案数据的信息
* 识别器性能可能向上

#### 不适合的数据 (P7)
* 不能明确分类
* 识别边界的位置存在很大的不同
* 识别器的性能可能下降

#### 半教師あり学習が可能な data の仮定 (P8)
* 平滑性假定
  * 输入的 x1, x2 在高密度区域的话，输出的 y1, y2 有关联
* cluster 假定
* 低密度分离
  * 识别分界在低密度区域
* 多样性假定

### 2. category 特征的情况 (P9)
* 类别特征的学习数据能够大量输入的问题，基本上都是输入语言数据

#### 适合的数据 (P9)
* 有正确答案的数据可以抽出很多特征语，且没有正确答案的数据也都包含这些特征语
* 对识别有用的特征语的 overlap(重叠) 很多的数据、即使没有正解，也可以高精度预测

#### Overlap 的传播 (P10)
* 有正解数据的特征语和 overlap 很多。无正解数据会变为新的有正解数据
* 新的有正解数据的特征语加入到特征语集合中
* 半教師あり学習比较适合「文書分類問題」

### 半教師あり学習 Algorithm (P11)
* 基本思想: 用无正解的数据，去调整通过有正解数据生成的识别器的参数


## 自己学习 Self-Training (P13)
* 在用有正解数据做出来的识别器的输出中，相信高可信度的结果，并重复将数据纳入有正解数据的集合中，重新训练自己
* 不满足低密度分离的数据，会影响识别器 (P13)

## 共训练 Co-Training (P14)
* 创建两个具有不同特征的分类器，并利用彼此的识别结果来训练每个分类器
* 两个具有不同判断标准的分类器相互学习
* 优点
  * 在初期对错误有很强的抵抗力
* 缺点
  * 不一样的特征集合很难找
  * 自己学习可能性能更好
* 共训练 Algorithm (P15)

## YATSI Algorithm Yet Another Two-Stage Idea (P16)
* 识别器只学习有正解数据一次，然后识别所有无正解数据，结果结合权重，通过 k-NN 法
* 旨在避免由于迭代算法中的重复而导致的错误放大
* 自己学习和共训练基本用于而分类问题，而 YATSI 适用于多分类问题
* YATSI Algorithm (P17)

## Label 传播法 (P18)
* data 看作是 node，基于类似度，构造 graph 结构
* 假设附近的节点可能属于同一类，预测无正解数据
* 主要是将评价函数 J(**f**) 最小化 (P18)
* 学習手順 (P19)
  * 1. 基于数据间的类似度，构建 graph
    * 标准
      * Gauss kernel: 所有节点都被连接，并赋予一个连续的结合值
      * k-NN法: 只连接附近 k 个节点
  * 2. 最小化评价函数
    * 重复进行 ”从有正解 node 向无正解 node 传播 label“ 的操作，通过最小化评价函数，进行优化使得邻接 node 具有相同的 label 